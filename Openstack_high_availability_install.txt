
Contents

Overview
Environment
PreInstall  on all nodes
HAProxy  on all controller nodes
Galera  on all controller nodes
RabbitMQ  on all controller nodes
Memcached  on all controller nodes
Redis  on all controller nodes
Keepalived  on all controller nodes
Keystone  on all controller nodes
Glance  on all controller nodes
Neutron  on all controller nodes
Nova  on all controller nodes
Horizon  on all controller nodes
Compute  on all compute nodes
Test case


----------------------------------------------------------
----------------------------------------------------------
Overview
----------------------------------------------------------
----------------------------------------------------------

Openstack高可用相关概念：
无状态的服务，例如nova-api，多节点部署，负载均衡方式访问(haproxy)
有状态的服务，例如mysql/rabbitmq，多节点部署，组件自身的机制来实现数据多节点保持一致，通过VIP方式访问(keepalived/pacemaker)
http://docs.openstack.org/ha-guide/intro-ha-concepts.html

本文高可用部署配置主要参考下面的架构。
https://github.com/beekhof/osp-ha-deploy/blob/master/HA-keepalived.md
1. 采用keepalived+haproxy搭建。
通过参考相关资料和讨论，keepalived相对pacemaker简单，pacemaker可以提供复杂的控制（启动停止切换服务／挂载文件系统等）。
https://www.reddit.com/r/linuxadmin/comments/35jmme/best_ip_failover_tool/
https://ask.openstack.org/en/question/84389/openstack-high-availability-pacemaker-or-keepalived/
openstack的官方文档目前采用pacemaker来实施高可用，后续如果需要针对Service级别的灵活控制，需要熟悉测试。

2. 网络采用LinuxBridge可控一些。

3. 目前只涉及openstack核心基础部件的高可用，mysql，rabbitmq，redis，keystone，glance，neutron，nova，horizon。

4. keepalived存在脑裂的架构限制，后续可以通过独立的监控告警来规避。

5. glance镜像存储高可用需要采用NFS或者其它共享存储，本操作指导暂时没有涉及。

6. 本次高可用环境搭建测试通过一台物理机器上的5台kvm虚拟机进行，硬件故障方面模拟测试较少。

----------------------------------------------------------
----------------------------------------------------------
Environment
----------------------------------------------------------
----------------------------------------------------------

http://controller-vip:9300/admin (root/pass)
http://controller-vip/dashboard (admin/pass , demo/pass)

controller1   
  eth0: 192.168.1.93
  eth1: no IP, used for provider network
  50 GB (/dev/vda)    
  100 GB (/dev/vdb)
controller2
  eth0: 192.168.1.94
  eth1: no IP, used for provider network
  50 GB (/dev/vda)    
  100 GB (/dev/vdb)
controller3
  eth0: 192.168.1.95
  eth1: no IP, used for provider network
  50 GB (/dev/vda)    
  100 GB (/dev/vdb)
controller-vip (virtual hostname)
  eth0: 192.168.1.98    

compute1  
  eth0: 192.168.1.96
  eth1: no IP, used for provider network
  50 GB (/dev/vda)
  100 GB (/dev/vdb)
compute2
  eth0: 192.168.1.97
  eth1: no IP, used for provider network
  50 GB (/dev/vda)
  100 GB (/dev/vdb)

OS version: CentOS 7.2
OpenStack version: Liberty

----------------------------------------------------------
----------------------------------------------------------
PreInstall  on all nodes
----------------------------------------------------------
----------------------------------------------------------

vi /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
ONBOOT=yes
BOOTPROTO=static
IPADDR=192.168.1.9X
NETMASK=255.255.255.0
GATEWAY=192.168.1.1

# public network ==> direct link to internet
vi /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
TYPE=Ethernet
BOOTPROTO=none
ONBOOT=yes

systemctl disable NetworkManager
systemctl stop NetworkManager
chkconfig network on
systemctl restart network

echo "nameserver 8.8.8.8" > /etc/resolv.conf

cat >> /etc/hosts << OFF
192.168.1.93    controller1
192.168.1.94    controller2
192.168.1.95    controller3
192.168.1.96    compute1
192.168.1.97    compute2
192.168.1.98    controller-vip
OFF

yum clean all
yum -y update

systemctl stop chronyd.service
systemctl disable chronyd.service
yum erase -y chrony
rm -f /etc/chrony*

yum install net-tools sysstat tcpdump wget ntp ntpdate -y

sed -i -e 's/=enforcing/=disabled/g' /etc/sysconfig/selinux
sed -i -e 's/=enforcing/=disabled/g' /etc/selinux/config

systemctl stop firewalld
systemctl disable firewalld

systemctl stop postfix.service
systemctl disable postfix.service

timedatectl set-timezone Asia/Shanghai

systemctl enable ntpd
systemctl start ntpd
ntpq -p  # Verify operation

reboot


yum install epel-release -y
yum install centos-release-openstack-liberty -y
yum upgrade


----------------------------------------------------------
----------------------------------------------------------
HAProxy  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install packages
yum install -y haproxy openstack-selinux

# Allow binding to non-local IPs
echo net.ipv4.ip_nonlocal_bind=1 >> /etc/sysctl.d/haproxy.conf
echo 1 > /proc/sys/net/ipv4/ip_nonlocal_bind

# Configure HAProxy
cat > /etc/haproxy/haproxy.cfg << EOF
global
    daemon
    stats socket /var/lib/haproxy/stats
defaults
    mode tcp
    maxconn 10000
    timeout connect 5s
    timeout client 30s
    timeout server 30s

listen monitor
    bind 192.168.1.98:9300 
    mode http
    monitor-uri /status
    stats enable
    stats uri /admin
    stats realm Haproxy\ Statistics
    stats auth root:pass
    stats refresh 5s

frontend vip-db
    bind 192.168.1.98:3306
    timeout client 90m
    default_backend db-vms-galera
backend db-vms-galera
    option httpchk
    stick-table type ip size 1000
    stick on dst
    timeout server 90m
    server openstack-node1 192.168.1.93:3306 check inter 1s port 9200 backup on-marked-down shutdown-sessions
    server openstack-node2 192.168.1.94:3306 check inter 1s port 9200 backup on-marked-down shutdown-sessions
    server openstack-node3 192.168.1.95:3306 check inter 1s port 9200 backup on-marked-down shutdown-sessions

# Note the RabbitMQ entry is only needed for CloudForms compatibility
# and should be removed in the future
frontend vip-rabbitmq
    option clitcpka
    bind 192.168.1.98:5672
    timeout client 900m
    default_backend rabbitmq-vms
backend rabbitmq-vms
    option srvtcpka
    balance roundrobin
    timeout server 900m
    server openstack-node1 192.168.1.93:5672 check inter 1s
    server openstack-node2 192.168.1.94:5672 check inter 1s
    server openstack-node3 192.168.1.95:5672 check inter 1s

frontend vip-keystone-admin
    bind 192.168.1.98:35357
    default_backend keystone-admin-vms
    timeout client 600s
backend keystone-admin-vms
    balance roundrobin
    timeout server 600s
    server openstack-node1 192.168.1.93:35357 check inter 1s on-marked-down shutdown-sessions
    server openstack-node2 192.168.1.94:35357 check inter 1s on-marked-down shutdown-sessions
    server openstack-node3 192.168.1.95:35357 check inter 1s on-marked-down shutdown-sessions

frontend vip-keystone-public
    bind 192.168.1.98:5000
    default_backend keystone-public-vms
    timeout client 600s
backend keystone-public-vms
    balance roundrobin
    timeout server 600s
    server openstack-node1 192.168.1.93:5000 check inter 1s on-marked-down shutdown-sessions
    server openstack-node2 192.168.1.94:5000 check inter 1s on-marked-down shutdown-sessions
    server openstack-node3 192.168.1.95:5000 check inter 1s on-marked-down shutdown-sessions

frontend vip-glance-api
    bind 192.168.1.98:9191
    default_backend glance-api-vms
backend glance-api-vms
    balance roundrobin
    server openstack-node1 192.168.1.93:9191 check inter 1s
    server openstack-node2 192.168.1.94:9191 check inter 1s
    server openstack-node3 192.168.1.95:9191 check inter 1s

frontend vip-glance-registry
    bind 192.168.1.98:9292
    default_backend glance-registry-vms
backend glance-registry-vms
    balance roundrobin
    server openstack-node1 192.168.1.93:9292 check inter 1s
    server openstack-node2 192.168.1.94:9292 check inter 1s
    server openstack-node3 192.168.1.95:9292 check inter 1s

frontend vip-neutron
    bind 192.168.1.98:9696
    default_backend neutron-vms
backend neutron-vms
    balance roundrobin
    server openstack-node1 192.168.1.93:9696 check inter 1s
    server openstack-node2 192.168.1.94:9696 check inter 1s
    server openstack-node3 192.168.1.95:9696 check inter 1s

frontend vip-nova-vnc-novncproxy
    bind 192.168.1.98:6080
    default_backend nova-vnc-novncproxy-vms
backend nova-vnc-novncproxy-vms
    balance roundrobin
    timeout tunnel 1h
    server openstack-node1 192.168.1.93:6080 check inter 1s
    server openstack-node2 192.168.1.94:6080 check inter 1s
    server openstack-node3 192.168.1.95:6080 check inter 1s

frontend nova-metadata-vms
    bind 192.168.1.98:8775
    default_backend nova-metadata-vms
backend nova-metadata-vms
    balance roundrobin
    server openstack-node1 192.168.1.93:8775 check inter 1s
    server openstack-node2 192.168.1.94:8775 check inter 1s
    server openstack-node3 192.168.1.95:8775 check inter 1s

frontend vip-nova-api
    bind 192.168.1.98:8774
    default_backend nova-api-vms
backend nova-api-vms
    balance roundrobin
    server openstack-node1 192.168.1.93:8774 check inter 1s
    server openstack-node2 192.168.1.94:8774 check inter 1s
    server openstack-node3 192.168.1.95:8774 check inter 1s

frontend vip-horizon
    bind 192.168.1.98:80
    timeout client 180s
    default_backend horizon-vms
backend horizon-vms
    balance roundrobin
    timeout server 180s
    mode http
    cookie SERVERID insert indirect nocache
    server openstack-node1 192.168.1.93:80 check inter 1s cookie openstack-horizon1 on-marked-down shutdown-sessions
    server openstack-node2 192.168.1.94:80 check inter 1s cookie openstack-horizon2 on-marked-down shutdown-sessions
    server openstack-node3 192.168.1.95:80 check inter 1s cookie openstack-horizon3 on-marked-down shutdown-sessions
EOF


# Note we are not starting haproxy yet.
#  http://controller-vip:9300/admin (root/pass).


----------------------------------------------------------
----------------------------------------------------------
Galera  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y mariadb-galera-server xinetd rsync

# Configure cluster check
cat > /etc/sysconfig/clustercheck << EOF
MYSQL_USERNAME="clustercheck"
MYSQL_PASSWORD="pass"
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
EOF

# Start mysqld and create user for cluster check
systemctl start mariadb
mysql -e "CREATE USER 'clustercheck'@'localhost' IDENTIFIED BY 'pass';"
systemctl stop mariadb

# Create Galera configuration
# replace bind_address to local IP !!!!!
cat > /etc/my.cnf.d/galera.cnf << EOF
[mysqld]
skip-name-resolve=1
binlog_format=ROW
default-storage-engine=innodb
innodb_autoinc_lock_mode=2
innodb_locks_unsafe_for_binlog=1
max_connections=8192
query_cache_size=0
query_cache_type=0
bind_address=192.168.1.9X
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so
wsrep_cluster_name="galera_cluster"
wsrep_cluster_address="gcomm://192.168.1.93,192.168.1.94,192.168.1.95"
wsrep_slave_threads=1
wsrep_certify_nonPK=1
wsrep_max_ws_rows=131072
wsrep_max_ws_size=1073741824
wsrep_debug=0
wsrep_convert_LOCK_to_trx=0
wsrep_retry_autocommit=1
wsrep_auto_increment_control=1
wsrep_drupal_282555_workaround=0
wsrep_causal_reads=0
wsrep_notify_cmd=
wsrep_sst_method=rsync
EOF

# Create Galera systemd configuration file, to allow a higher number of files to be opened
mkdir -p /etc/systemd/system/mariadb.service.d/
cat > /etc/systemd/system/mariadb.service.d/limits.conf << EOF
[Service]
LimitNOFILE=16384
EOF

# Configure monitor service (used by HAProxy)
cat > /etc/xinetd.d/galera-monitor << EOF
service galera-monitor
{
    port = 9200
    disable = no
    socket_type = stream
    protocol = tcp
    wait = no
    user = root
    group = root
    groups = yes
    server = /usr/bin/clustercheck
    type = UNLISTED
    per_source = UNLIMITED
    log_on_success = 
    log_on_failure = HOST
    flags = REUSE
}
EOF

# Start services
systemctl daemon-reload
systemctl enable xinetd
systemctl start xinetd
systemctl enable haproxy
systemctl start haproxy

# Start mariadb cluster
On all nodes:
systemctl enable mariadb
On node1:
sudo -u mysql /usr/libexec/mysqld --wsrep-cluster-address='gcomm://' &
mysql >
 SHOW GLOBAL STATUS LIKE 'wsrep%';
 SHOW STATUS LIKE 'wsrep_cluster_size';
 quit
clustercheck

On node2 and node3:
systemctl start mariadb
Once clustercheck returns 200 on all nodes, restart galera on node 1:
kill <mysql PIDs>
systemctl start mariadb

# Check
mysql -u root -p -e "show status like 'wsrep%'"
wsrep_cluster_size            3  # number of nodes
wsrep_connected               ON
wsrep_ready                   ON   # It's running,awesome !
wsrep_incoming_addresses      | 192.168.1.93:3306,192.168.1.94:3306,192.168.1.95:3306

# Create users and databases
On node 1:
mysql

MariaDB [(none)]> 
use mysql;
drop user ''@'controller1';
drop user 'root'@'controller1';
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED by 'pass' WITH GRANT OPTION;
CREATE DATABASE keystone;
GRANT ALL ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'pass';
CREATE DATABASE glance;
GRANT ALL ON glance.* TO 'glance'@'%' IDENTIFIED BY 'pass';
CREATE DATABASE neutron;
GRANT ALL ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'pass';
CREATE DATABASE nova;
GRANT ALL ON nova.* TO 'nova'@'%' IDENTIFIED BY 'pass';
FLUSH PRIVILEGES;
quit

mysqladmin flush-hosts


问题：
1. systemctl start mariadb 启动不成功，提示mysql.user等没有创建
rm -rf /var/lib/mysql/*
mysql_install_db
chown -R mysql:mysql /var/lib/mysql
2. 节点1的集群首次初始化也可以用下面命令。
sudo -u mysql /usr/libexec/mysqld --wsrep-new-cluster &

----------------------------------------------------------
----------------------------------------------------------
RabbitMQ  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install package
yum -y install rabbitmq-server

# Create erlang cookie and distribute
On node 1:
cat > /etc/rabbitmq/rabbitmq-env.conf << EOF
NODE_IP_ADDRESS=192.168.1.93
EOF

systemctl start rabbitmq-server
systemctl stop rabbitmq-server
scp -p /var/lib/rabbitmq/.erlang.cookie controller2:/var/lib/rabbitmq
scp -p /var/lib/rabbitmq/.erlang.cookie controller3:/var/lib/rabbitmq

# Set permissions for erlang cookie
# replace ip to local IP !!!!!
On node 2 and node 3:
chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie

cat > /etc/rabbitmq/rabbitmq-env.conf << EOF
NODE_IP_ADDRESS=192.168.1.9X
EOF

# Create rabbitmq configuration
On all nodes:
cat > /etc/rabbitmq/rabbitmq.config << EOF

[
    {rabbit, [
    {cluster_nodes, {['rabbit@controller1', 'rabbit@controller2', 'rabbit@controller3'], disc}},
    {cluster_partition_handling, ignore},
    {default_user, <<"guest">>},
    {default_pass, <<"guest">>},
    {tcp_listen_options, [binary,
        {packet, raw},
        {reuseaddr, true},
        {backlog, 128},
        {nodelay, true},
        {exit_on_close, false},
        {keepalive, true}]}
    ]},
    {kernel, [
        {inet_dist_listen_max, 44001},
        {inet_dist_listen_min, 44001}
    ]}
].

EOF

# Set kernel TCP keepalive parameters
cat > /etc/sysctl.d/tcpka.conf << EOF
net.ipv4.tcp_keepalive_intvl = 1
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_time = 5
EOF

sysctl -p /etc/sysctl.d/tcpka.conf

# Start services
systemctl enable rabbitmq-server
systemctl start rabbitmq-server

rabbitmqctl cluster_status

# Set HA mode for all queues
On node 1:
rabbitmqctl set_policy HA '^(?!amq\.).*' '{"ha-mode": "all"}'


----------------------------------------------------------
----------------------------------------------------------
Memcached  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install and enable memcached
yum install -y memcached
systemctl start memcached
systemctl enable memcached

----------------------------------------------------------
----------------------------------------------------------
Redis  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install redis
yum install -y redis

# Configure bind IP, set master and slaves
# replace ip to local IP or <controller1 IP> !!!!!
On all nodes:
sed --in-place 's/bind 127.0.0.1/bind 127.0.0.1 192.168.1.9X/' /etc/redis.conf

On node2 and 3:
echo slaveof ''<controller1 IP>'' 6379 >> /etc/redis.conf

# Configure Sentinel, used for master failover
On all nodes:
cat > /etc/redis-sentinel.conf << EOF

sentinel monitor mymaster <controller1 IP> 6379 2
sentinel down-after-milliseconds mymaster 30000
sentinel failover-timeout mymaster 180000
sentinel parallel-syncs mymaster 1
min-slaves-to-write 1
min-slaves-max-lag 10
logfile /var/log/redis/sentinel.log
EOF

# Start services
systemctl enable redis
systemctl start redis
systemctl enable redis-sentinel
systemctl start redis-sentinel


----------------------------------------------------------
----------------------------------------------------------
Keepalived  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum -y install keepalived psmisc

# Create configuration file
On all nodes:
cat > /etc/keepalived/keepalived.conf << EOF

vrrp_script chk_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
}

vrrp_instance VI_PUBLIC {
    interface eth0
    state BACKUP
    virtual_router_id 52
    priority 101
    virtual_ipaddress {
        192.168.1.98 dev eth0
    }
    track_script {
        chk_haproxy
    }
    # Avoid failback
    nopreempt
}

vrrp_sync_group VG1
    group {
        VI_PUBLIC
    }
EOF

# Start services
On all nodes:
systemctl start keepalived
systemctl enable keepalived


----------------------------------------------------------
----------------------------------------------------------
Keystone  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y openstack-keystone openstack-utils openstack-selinux httpd mod_wsgi python-openstackclient

# Create service token and distribute to the other controllers
On node 1:
export SERVICE_TOKEN=$(openssl rand -hex 10)
echo $SERVICE_TOKEN > /root/keystone_service_token
scp /root/keystone_service_token root@controller2:/root
scp /root/keystone_service_token root@controller3:/root

# Configure Apache web server for Keystone
# replace ip to local IP !!!!!
On all nodes:
cp /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
sed -i -e 's/apache2/httpd/g'   /etc/httpd/conf.d/wsgi-keystone.conf
sed -i -e 's/VirtualHost \*/VirtualHost 192.168.1.9X/g' /etc/httpd/conf.d/wsgi-keystone.conf 
sed -i -e 's/Listen 5000/Listen 192.168.1.9X:5000/g' /etc/httpd/conf.d/wsgi-keystone.conf 
sed -i -e 's/Listen 35357/Listen 192.168.1.9X:35357/g' /etc/httpd/conf.d/wsgi-keystone.conf 
sed -i -e 's/^Listen.*/Listen 192.168.1.9X:80/g' /etc/httpd/conf/httpd.conf 

# Configure Keystone
On all nodes:
export SERVICE_TOKEN=$(cat /root/keystone_service_token)
openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token $SERVICE_TOKEN
openstack-config --set /etc/keystone/keystone.conf DEFAULT rabbit_hosts controller1,controller2,controller3
openstack-config --set /etc/keystone/keystone.conf DEFAULT rabbit_ha_queues true
openstack-config --set /etc/keystone/keystone.conf eventlet_server admin_endpoint 'http://controller-vip:35357/'
openstack-config --set /etc/keystone/keystone.conf eventlet_server public_endpoint 'http://controller-vip:5000/'
openstack-config --set /etc/keystone/keystone.conf database connection mysql://keystone:pass@controller-vip/keystone
openstack-config --set /etc/keystone/keystone.conf database max_retries -1
openstack-config --set /etc/keystone/keystone.conf DEFAULT public_bind_host 192.168.1.9X
openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_bind_host 192.168.1.9X
openstack-config --set /etc/keystone/keystone.conf token driver  keystone.token.persistence.backends.sql.Token

# Create and distribute PKI setup, manage DB
On node 1:
keystone-manage pki_setup --keystone-user keystone --keystone-group keystone
chown -R keystone:keystone /var/log/keystone /etc/keystone/ssl/

su keystone -s /bin/sh -c "keystone-manage db_sync"

cd /etc/keystone/ssl
tar cvfz /tmp/keystone_ssl.tgz *
scp /tmp/keystone_ssl.tgz controller2:/tmp
scp /tmp/keystone_ssl.tgz controller3:/tmp

# Restore Keystone PKI setup from node 1
On nodes 2 and 3:
mkdir -p /etc/keystone/ssl
cd /etc/keystone/ssl
tar xvfz /tmp/keystone_ssl.tgz 
chown -R keystone:keystone /var/log/keystone /etc/keystone/ssl/
restorecon -Rv /etc/keystone/ssl
touch /var/log/keystone/keystone.log
chown keystone:keystone /var/log/keystone/keystone.log

# Create cron job to flush expired tokens
On all nodes:
echo "1 * * * * keystone keystone-manage token_flush >>/var/log/keystone/keystone-tokenflush.log 2>&1" >> /etc/crontab

# Start services
On all nodes:
systemctl start httpd
systemctl enable httpd

# Create endpoints, services and users for all API services
On node 1:
export OS_TOKEN=$(cat /root/keystone_service_token)
export OS_URL=http://controller-vip:35357/v2.0
export OS_REGION_NAME=regionOne
openstack service create --name=keystone --description="Keystone Identity Service" identity
openstack endpoint create --publicurl 'http://controller-vip:5000/v2.0' --adminurl 'http://controller-vip:35357/v2.0' --internalurl 'http://controller-vip:5000/v2.0' --region regionOne keystone
openstack user create --password pass admin
openstack role create admin
openstack project create admin
openstack role add --project admin --user admin admin
openstack user create --password pass demo
openstack role create user
openstack project create demo
openstack role add --project demo --user demo user
openstack project create --description "Services Tenant" services
# glance
openstack user create --password pass glance
openstack role add --project services --user glance admin
openstack service create --name=glance --description="Glance Image Service" image
openstack endpoint create --publicurl 'http://controller-vip:9292' --adminurl 'http://controller-vip:9292' --internalurl 'http://controller-vip:9292' --region regionOne glance
# neutron
openstack user create --password pass neutron
openstack role add --project services --user neutron admin
openstack service create --name=neutron --description="OpenStack Networking Service" network
openstack endpoint create --publicurl "http://controller-vip:9696" --adminurl "http://controller-vip:9696" --internalurl "http://controller-vip:9696" --region regionOne neutron
# nova
openstack user create --password pass compute
openstack role add --project services --user compute admin
openstack service create --name=compute --description="OpenStack Compute Service" compute
openstack endpoint create --publicurl "http://controller-vip:8774/v2/\$(tenant_id)s" --adminurl "http://controller-vip:8774/v2/\$(tenant_id)s" --internalurl "http://controller-vip:8774/v2/\$(tenant_id)s" --region regionOne compute

# Create keystonerc files for simplicity
On all nodes:
cat > /root/keystonerc_admin << EOF
export OS_USERNAME=admin 
export OS_TENANT_NAME=admin
export OS_PROJECT_NAME=admin
export OS_REGION_NAME=regionOne
export OS_PASSWORD=pass
export OS_AUTH_URL=http://controller-vip:35357/v2.0/
EOF

cat > /root/keystonerc_demo << EOF
export OS_USERNAME=demo
export OS_TENANT_NAME=demo
export OS_PROJECT_NAME=demo
export OS_REGION_NAME=regionOne
export OS_PASSWORD=pass
export OS_AUTH_URL=http://controller-vip:5000/v2.0/
EOF


----------------------------------------------------------
----------------------------------------------------------
Glance  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y openstack-glance openstack-utils openstack-selinux nfs-utils

# Configure glance-api and glance-registry
openstack-config --set /etc/glance/glance-api.conf database connection mysql://glance:pass@controller-vip/glance
openstack-config --set /etc/glance/glance-api.conf database max_retries -1
openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller-vip:5000/
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_plugin password
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller-vip:35357/
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password pass
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name services
openstack-config --set /etc/glance/glance-api.conf DEFAULT notification_driver messaging

# replace ip to local IP !!!!!
openstack-config --set /etc/glance/glance-api.conf DEFAULT bind_host 192.168.1.9X

openstack-config --set /etc/glance/glance-api.conf DEFAULT registry_host controller-vip
openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_hosts controller1,controller2,controller3
openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_ha_queues true
openstack-config --set /etc/glance/glance-registry.conf database connection mysql://glance:pass@controller-vip/glance
openstack-config --set /etc/glance/glance-registry.conf database max_retries -1
openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller-vip:5000/
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_plugin password
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller-vip:35357/
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password pass
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name services

openstack-config --set /etc/glance/glance-registry.conf DEFAULT bind_host 192.168.1.9X

# Manage DB
On node 1:
su glance -s /bin/sh -c "glance-manage db_sync"

# Configure backend
Glance高可用要求使用NFS等共享存储，暂时不配置NFS，测试镜像在3个控制节点的/var/lib/glance目录同步下

# Start services
systemctl start openstack-glance-registry
systemctl start openstack-glance-api
systemctl enable openstack-glance-registry
systemctl enable openstack-glance-api


# Test
On any node:
. /root/keystonerc_admin
wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
glance image-create --name "cirros" --disk-format qcow2 --container-format bare --file cirros-0.3.4-x86_64-disk.img --visibility public
glance image-list


----------------------------------------------------------
----------------------------------------------------------
Neutron  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y openstack-neutron openstack-neutron-linuxbridge openstack-neutron-ml2 openstack-utils openstack-selinux

# Configure Neutron server
# replace ip to local IP !!!!!
openstack-config --set /etc/neutron/neutron.conf DEFAULT bind_host 192.168.1.9X

openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller-vip:5000/
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_plugin password
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller-vip:35357/
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password pass
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name services

openstack-config --set /etc/neutron/neutron.conf database connection mysql://neutron:pass@controller-vip:3306/neutron
openstack-config --set /etc/neutron/neutron.conf database max_retries -1

openstack-config --set /etc/neutron/neutron.conf DEFAULT notification_driver neutron.openstack.common.notifier.rpc_notifier
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_hosts controller1,controller2,controller3
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_ha_queues true

openstack-config --set /etc/neutron/neutron.conf nova nova_region_name regionOne
openstack-config --set /etc/neutron/neutron.conf nova project_domain_id default
openstack-config --set /etc/neutron/neutron.conf nova project_name services
openstack-config --set /etc/neutron/neutron.conf nova user_domain_id default
openstack-config --set /etc/neutron/neutron.conf nova password pass
openstack-config --set /etc/neutron/neutron.conf nova username compute
openstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller-vip:35357/
openstack-config --set /etc/neutron/neutron.conf nova auth_plugin password
openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes True
openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes True

openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin neutron.plugins.ml2.plugin.Ml2Plugin
openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
openstack-config --set /etc/neutron/neutron.conf DEFAULT router_scheduler_driver neutron.scheduler.l3_agent_scheduler.ChanceScheduler
openstack-config --set /etc/neutron/neutron.conf DEFAULT dhcp_agents_per_network 2
openstack-config --set /etc/neutron/neutron.conf DEFAULT api_workers 2
openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_workers 2
openstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha True
openstack-config --set /etc/neutron/neutron.conf DEFAULT min_l3_agents_per_router 2
openstack-config --set /etc/neutron/neutron.conf DEFAULT max_l3_agents_per_router 2

# ML2 configuration
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers linuxbridge,l2population
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks provider
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vlan network_vlan_ranges provider
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 100:1000
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset  True

# Linuxbridge configuration
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth1

openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan  True

# replace ip to local IP !!!!!
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan local_ip  192.168.1.9x

openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan l2_population True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini agent prevent_arp_spoofing True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver


# Manage DB
On node 1:
neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head

# Start services
On all nodes:
systemctl start neutron-server
systemctl enable neutron-server


# Metadata agent
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT auth_url http://controller-vip:35357/v2.0
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT auth_host controller-vip
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT auth_region regionOne
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT admin_tenant_name services
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT admin_user neutron
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT admin_password pass
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip controller-vip
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_port 8775
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret metatest
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_workers 4
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_backlog 2048

# DHCP agent
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriver
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq-neutron.conf
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT verbose True

The following will prevent issues from happening when the network card MTU is 1500. If we are using jumbo frames, it should not be required. Be aware that this only helps on certain operating systems with a well-behaving DHCP client. Windows is known to ignore it.
echo "dhcp-option-force=26,1400" > /etc/neutron/dnsmasq-neutron.conf
chown root:neutron /etc/neutron/dnsmasq-neutron.conf
chmod 644 /etc/neutron/dnsmasq-neutron.conf

# L3 agent
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriver
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT handle_internal_only_routers True
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT send_arp_for_ha 3
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT metadata_ip controller-vip
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge

# Start services
systemctl start neutron-linuxbridge-agent
systemctl start neutron-dhcp-agent
systemctl start neutron-l3-agent
systemctl start neutron-metadata-agent
systemctl enable neutron-linuxbridge-agent
systemctl enable neutron-dhcp-agent
systemctl enable neutron-l3-agent
systemctl enable neutron-metadata-agent


mkdir -p /etc/systemd/system/neutron-server.service.d
cat > /etc/systemd/system/neutron-server.service.d/restart.conf << EOF
[Service]
Restart=on-failure
EOF


# Create provider network
. /root/keystonerc_admin
neutron net-create public --shared --provider:network_type flat --provider:physical_network provider --router:external
neutron subnet-create --gateway 192.168.100.1 --allocation-pool start=192.168.100.120,end=192.168.100.200 --name public_subnet public 192.168.100.0/24

# Create the private project network
. /root/keystonerc_admin
openstack project show demo
neutron net-create private --tenant-id 3a6ff61135054cb5a18c90aefdece64c \
  --provider:network_type vxlan
. /root/keystonerc_demo
neutron subnet-create private 10.0.0.0/24 --name private \
  --dns-nameserver 114.114.114.114 --gateway 10.0.0.1


# Create a router
. /root/keystonerc_admin
neutron net-update public --router:external
. /root/keystonerc_demo
neutron router-create router
neutron router-interface-add router private
neutron router-gateway-set router public

----------------------------------------------------------
----------------------------------------------------------
Nova  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y openstack-nova-console openstack-nova-novncproxy openstack-utils openstack-nova-api openstack-nova-conductor openstack-nova-scheduler python-cinderclient python-memcached

# Configure Nova API
# replace ip to local IP !!!!!
openstack-config --set /etc/nova/nova.conf DEFAULT memcached_servers controller1:11211,controller2:11211,controller3:11211

openstack-config --set /etc/nova/nova.conf DEFAULT novncproxy_host 192.168.1.9X

openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller-vip:6080/vnc_auto.html

openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.1.9X
openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.1.9X

openstack-config --set /etc/nova/nova.conf database connection mysql://nova:pass@controller-vip/nova
openstack-config --set /etc/nova/nova.conf database max_retries -1
openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone

openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.1.9X
openstack-config --set /etc/nova/nova.conf DEFAULT metadata_host 192.168.1.9X
openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.1.9X

openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen_port 8775
openstack-config --set /etc/nova/nova.conf DEFAULT glance_host controller-vip
openstack-config --set /etc/nova/nova.conf DEFAULT network_api_class nova.network.neutronv2.api.API
openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf libvirt vif_driver nova.virt.libvirt.vif.LibvirtGenericVIFDriver
openstack-config --set /etc/nova/nova.conf DEFAULT security_group_api neutron
openstack-config --set /etc/nova/nova.conf cinder cinder_catalog_info volume:cinder:internalURL
openstack-config --set /etc/nova/nova.conf conductor use_local false
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_hosts controller1,controller2,controller3
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_ha_queues True

openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True
openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret metatest
openstack-config --set /etc/nova/nova.conf neutron url http://controller-vip:9696/
openstack-config --set /etc/nova/nova.conf neutron project_domain_id default
openstack-config --set /etc/nova/nova.conf neutron project_name services
openstack-config --set /etc/nova/nova.conf neutron user_domain_id default
openstack-config --set /etc/nova/nova.conf neutron username neutron
openstack-config --set /etc/nova/nova.conf neutron password pass
openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller-vip:35357/
openstack-config --set /etc/nova/nova.conf neutron auth_uri http://controller-vip:5000/
openstack-config --set /etc/nova/nova.conf neutron auth_plugin password
openstack-config --set /etc/nova/nova.conf neutron region_name regionOne

# REQUIRED FOR A/A scheduler
openstack-config --set /etc/nova/nova.conf DEFAULT scheduler_host_subset_size 30
openstack-config --set /etc/nova/api-paste.ini filter:authtoken auth_plugin password
openstack-config --set /etc/nova/api-paste.ini filter:authtoken auth_url http://controller-vip:35357/
openstack-config --set /etc/nova/api-paste.ini filter:authtoken username compute
openstack-config --set /etc/nova/api-paste.ini filter:authtoken password pass
openstack-config --set /etc/nova/api-paste.ini filter:authtoken project_name services
openstack-config --set /etc/nova/api-paste.ini filter:authtoken auth_uri http://controller-vip:5000/

Only run the following command if you are creating a test environment where your hypervisors will be virtual machines
openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu

# Manage DB
On node 1:
su nova -s /bin/sh -c "nova-manage db sync"

# Start services
On all nodes:
systemctl start openstack-nova-consoleauth
systemctl start openstack-nova-novncproxy 
systemctl start openstack-nova-api
systemctl start openstack-nova-scheduler
systemctl start openstack-nova-conductor
systemctl enable openstack-nova-consoleauth
systemctl enable openstack-nova-novncproxy 
systemctl enable openstack-nova-api
systemctl enable openstack-nova-scheduler
systemctl enable openstack-nova-conductor


----------------------------------------------------------
----------------------------------------------------------
Horizon  on all controller nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y mod_wsgi httpd mod_ssl python-memcached openstack-dashboard

# Set secret key
On node 1:
openssl rand -hex 10

Take note of the generated random value, then on all nodes:
sed -i -e "s#SECRET_KEY.*#SECRET_KEY = 'VALUE'#g#" /etc/openstack-dashboard/local_settings
# sed -i -e "s#SECRET_KEY.*#SECRET_KEY = '7fc2927a3fa10229cd95'#g#" /etc/openstack-dashboard/local_settings

# Configure local_settings and httpd.conf
sed -i -e "s#ALLOWED_HOSTS.*#ALLOWED_HOSTS = ['*',]#g" \
-e "s#^CACHES#SESSION_ENGINE = 'django.contrib.sessions.backends.cache'\nCACHES#g#" \
-e "s#locmem.LocMemCache'#memcached.MemcachedCache',\n\t'LOCATION' : [ 'controller1:11211', 'controller2:11211', 'controller3:11211', ]#g" \
-e 's#OPENSTACK_HOST =.*#OPENSTACK_HOST = "controller-vip"#g' \
-e "s#^LOCAL_PATH.*#LOCAL_PATH = '/var/lib/openstack-dashboard'#g" \
/etc/openstack-dashboard/local_settings

# Restart httpd
systemctl daemon-reload
systemctl restart httpd



----------------------------------------------------------
----------------------------------------------------------
Compute  on all compute nodes
----------------------------------------------------------
----------------------------------------------------------

# Install software
yum install -y openstack-nova-compute openstack-utils python-cinder openstack-neutron-linuxbridge openstack-neutron openstack-selinux ebtables ipset


# Configure Nova compute
openstack-config --set /etc/nova/nova.conf DEFAULT memcached_servers controller1:11211,controller2:11211,controller3:11211

# replace ip to local IP !!!!!
openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.1.9X

openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller-vip:6080/vnc_auto.html
openstack-config --set /etc/nova/nova.conf database connection mysql://nova:pass@controller-vip/nova
openstack-config --set /etc/nova/nova.conf database max_retries -1

openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/nova/nova.conf glance host controller-vip
openstack-config --set /etc/nova/nova.conf DEFAULT network_api_class nova.network.neutronv2.api.API
openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf libvirt vif_driver nova.virt.libvirt.vif.LibvirtGenericVIFDriver
openstack-config --set /etc/nova/nova.conf DEFAULT security_group_api neutron
openstack-config --set /etc/nova/nova.conf cinder cinder_catalog_info volume:cinder:internalURL
openstack-config --set /etc/nova/nova.conf conductor use_local false

openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_hosts controller1,controller2,controller3
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_ha_queues True

openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True
openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret metatest
openstack-config --set /etc/nova/nova.conf neutron url http://controller-vip:9696/
openstack-config --set /etc/nova/nova.conf neutron project_domain_id default
openstack-config --set /etc/nova/nova.conf neutron project_name services
openstack-config --set /etc/nova/nova.conf neutron user_domain_id default
openstack-config --set /etc/nova/nova.conf neutron username neutron
openstack-config --set /etc/nova/nova.conf neutron password pass
openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller-vip:35357/
openstack-config --set /etc/nova/nova.conf neutron auth_uri http://controller-vip:5000/
openstack-config --set /etc/nova/nova.conf neutron auth_plugin password
openstack-config --set /etc/nova/nova.conf neutron region_name regionOne

openstack-config --set /etc/nova/nova.conf libvirt nfs_mount_options v3
openstack-config --set /etc/nova/api-paste.ini filter:authtoken auth_plugin password
openstack-config --set /etc/nova/api-paste.ini filter:authtoken auth_url http://controller-vip:35357/
openstack-config --set /etc/nova/api-paste.ini filter:authtoken username compute
openstack-config --set /etc/nova/api-paste.ini filter:authtoken password pass
openstack-config --set /etc/nova/api-paste.ini filter:authtoken project_name services
openstack-config --set /etc/nova/api-paste.ini filter:authtoken auth_uri http://controller-vip:5000/

Only run the following command if you are creating a test environment where your hypervisors will be virtual machines.
openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu

# Configure Neutron on compute node
openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller-vip:5000/
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_plugin password
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller-vip:35357/
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password pass
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name services

openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_hosts controller1,controller2,controller3
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_ha_queues true
openstack-config --set /etc/neutron/neutron.conf DEFAULT notification_driver neutron.openstack.common.notifier.rpc_notifier

openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth1
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan True

# replace ip to local IP !!!!!
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan local_ip 192.168.1.9X

openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan l2_population True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini agent prevent_arp_spoofing True 
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group True 
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.IptablesFirewallDriver


# Set kernel TCP keepalive parameters
cat > /etc/sysctl.d/tcpka.conf << EOF
net.ipv4.tcp_keepalive_intvl = 1
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_time = 5
EOF

sysctl -p /etc/sysctl.d/tcpka.conf


# Enable and start services
systemctl start libvirtd
systemctl enable libvirtd
systemctl start neutron-linuxbridge-agent
systemctl enable neutron-linuxbridge-agent
systemctl start openstack-nova-compute
systemctl enable openstack-nova-compute


----------------------------------------------------------
----------------------------------------------------------
Test case
----------------------------------------------------------
----------------------------------------------------------

1. 
场景：controller1 关机
结果：vip可以正常漂移，vm运行正常，vm ip(public and priate)正常访问，dashboard正常创建删除虚拟机。
恢复：controller1 开机即可。执行下面查询，各节点服务都正常。
     controller1:
     systemctl --failed # 检查没有启动失败的服务
     . /root/keystonerc_admin
     nova service-list
     nova list --all
     nuetron agent-list
     neutron net-list
     glance image-list

2.
场景：controller1，controller2 关机
结果：vip可以正常漂移，vm运行正常，vm ip(public and priate)正常访问，dashboard正常创建删除虚拟机。
恢复：controller1，controller2 开机即可。

3.
场景：controller1，controller2，controller3 全部控制节点关机
结果：计算节点vm运行正常，vm ip(public and priate)正常访问，dashboard不能访问。
恢复：controller1，controller2，controller3 开机。
     systemctl --failed # mariadb不能正常启动，需要按下面顺序启动。

     On controller1:
     sudo -u mysql /usr/libexec/mysqld --wsrep-cluster-address='gcomm://' &
     mysql >
      SHOW GLOBAL STATUS LIKE 'wsrep%';
      SHOW STATUS LIKE 'wsrep_cluster_size';
      quit

     On controller2 and controller3:
     systemctl start mariadb

     Once clustercheck returns 200 on all nodes, restart galera on node 1:
     clustercheck
     kill <mysql PIDs>
     systemctl start mariadb

4.
场景：controller3主机销毁
结果：同1
恢复：按照步骤重新安装controller3加入集群，生产环境需要建立本地源固定版本，避免版本变动导致的兼容问题。


结论：
经过测试，本文中openstack控制节点高可用部署可以达到预期目标。
1、任意控制节点down机，都不影响正常功能。
2、只有一个控制节点，openstack基本功能不受影响。
3、三个控制节点全部down机，计算节点的虚拟机不受影响，flat网络模式下虚拟机通信也没有影响，openstack基础管理功能全部失效。


----------------------------------------------------------
----------------------------------------------------------
Other
----------------------------------------------------------
----------------------------------------------------------

#cloud-config
chpasswd:
 list: |
   root:pass123
   centos:centos
 expire: False
ssh_pwauth: True


user:cirros  pass:cubswin:)
